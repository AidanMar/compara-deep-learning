import numpy as np
import pandas as pd

configfile:
    "config.json"


# get the list of species names for the homology databases
species = list(np.loadtxt(config["homology_species_list"], dtype="str"))


rule all:
	input:
		expand( "homology_databases/{SPECIES}.homologies.tsv.gz", SPECIES=species ),
		"downloads/gtfs",
		"downloads/cds",
		"downloads/pep",
		"genome_maps",
		directory("processed/pfam_matrices")
		
	


rule get_download_links:
	"""
	This rule runs the download links script. 
	This will get the user all of the possible 
	download links for the gtf, cds files and fasta files
	"""
	input:
		"scripts/ftpg.py"
	output:
		"download_links/gtf_link.txt",
		"download_links/seq_link.txt",
		"download_links/protein_seq.txt"
	script:
		"scripts/ftpg.py"

# read the list of seq links
seq_links = pd.Series(np.loadtxt("download_links/seq_link.txt", dtype="str"))
# Get the expected list of seq output files
seq_files = list("downloads/cds/" + seq_links.str.split("/").str[-1])

rule download_cds:
	input:
		"download_links/seq_link.txt"
	output:
		files = seq_files,
		out_dir = directory("downloads/cds"),
		log_path = "logs/seq_download_log.txt"
	shell:
		"scripts/downloader.sh {input} {output.out_dir} {output.log_path}"


# read the list of gtf links
gtf_links = pd.Series(np.loadtxt("download_links/gtf_link.txt", dtype="str"))
# Get the expected list of seq output files
gtf_files = list("downloads/gtfs/" + gtf_links.str.split("/").str[-1])
rule download_gtf:
	input:
		"download_links/gtf_link.txt"
	output:
		files = gtf_files,
		out_dir = directory("downloads/gtfs"),
		log_path = "logs/gtf_download_log.txt"
	shell:
		"scripts/downloader.sh {input} {output.out_dir} {output.log_path}"


# read the list of gtf links
pep_links = pd.Series(np.loadtxt("download_links/protein_seq.txt", dtype="str"))
# Get the expected list of seq output files
pep_files = list("downloads/pep/" + pep_links.str.split("/").str[-1])
rule download_pep:
	input:
		"download_links/protein_seq.txt"
	output:
		files = pep_files,
		out_dir = directory("downloads/pep"),
		log_path = "logs/pep_download_log.txt"
	shell:
		"scripts/downloader.sh {input} {output.out_dir} {output.log_path}"

# rule download_files:
# 	"""
# 	This rule runs the download links script. 
# 	This will get the user all of the possible 
# 	download links for the gtf, cds files and fasta files
# 	"""
# 	input:
# 		"download_links/gtf_link.txt",
# 		"download_links/seq_link.txt",
# 		"download_links/protein_seq.txt"
# 	output:
# 		"downloads/gtfs",
# 		"downloads/cds",
# 		"downloads/pep",
# 		"logs/gtf_download_log.txt",
# 		"logs/pep_download_log.txt",
# 		"logs/seq_download_log.txt"
# 	shell:
# 		"scripts/download_prelims.sh"



rule get_homology_databases:
	input:
		config["homology_species_list"]
	output:
		"homology_databases/{SPECIES}.homologies.tsv.gz"
	params:
		address=config["homology_database_ftp_address"]
		
	
	shell:
		"""
		# make the homology database directory
		mkdir -p homology_databases
		# This mysterious looking piece of code yanks the ftp path homology database for each species
		ftp_address=$(lynx -dump -listonly {params.address}{wildcards.SPECIES} | grep "protein" | tr -s ' ' | rev | cut -d " " -f 1 | rev)
		# Use that address to download file with appropriate name
		wget $ftp_address -q -O {output}
		
		"""

rule create_genome_map:
	input:
		files = gtf_files,
		in_dir = directory("downloads/gtfs")
	output:
		"genome_maps"
	shell:
		"python scripts/create_genome_map/create_genome_maps.py {input.in_dir} {output}"


# get the list of selected index file names we expect from homology databases
selected_index_files = list("processed/" + pd.Series(species)  + "_selected_indexes.npy")
rule select_homology_records:
	input:
		expand("homology_databases/{SPECIES}.homologies.tsv.gz", SPECIES = species),
		"scripts/dist_matrix",
		directory("homology_databases")
	output:
		directory("processed"),
		selected_index_files
	params:
		# The number of records to be selected from each file
		num_records = 1000
	shell:
		"""
		python scripts/select_data.py {params.num_records}
		"""
rule find_neighbor_genes:
	input:
		selected_index_files,
		directory("processed"),
		directory("homology_databases"),
		"genome_maps"
	output:
		"processed/neighbor_genes.json"
	script:
		"scripts/neighbor_genes.py"
		
	
rule prep_synteny:
	input:
		"processed/neighbor_genes.json",
		protein_files = pep_files,
		sequence_files = seq_files
	output:
		directory("processed/synteny_matrices"),
		"protein_seq_positive.fa"
	threads:
		4
	shell:
		"""
		python scripts/prepare_synteny_matrix.py {threads}
		"""

rule process_negatives:
	input:
		"protein_seq_positive.fa",
		"genome_maps",
		neg_samples = "negative_samples_50K.txt.gz",
		files = pep_files
	output:
		"processed/neighbor_genes_negative.json",
		"pro_seq_negative.fa"
	threads:
		4
	shell:
		"""
		# sometimes snakemake will execute this before other steps so we need to make the processed dir first, just in case
		mkdir -p processed

		python scripts/process_negative.py {input.neg_samples} {threads}
		"""

rule parse_pfam_domains:
	input:
		positive = "protein_seq_positive.fa",
		negative = "pro_seq_negative.fa"
	output:
		"pfam_db_positive.h5",
		"pfam_db_negative.h5"

	shell:
		"""
		python scripts/pfam_parser.py {input.positive} {input.negative}
		"""
	
rule pfam_matrix:
	input:
		"pfam_db_positive.h5",
		"pfam_db_negative.h5",
		"negative_samples_50K.txt.gz",
		directory("homology_databases")
	output:
		directory("processed/pfam_matrices")
	shell:
		"""
		python scripts/pfam_matrix.py {input}
		"""

